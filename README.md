# ParNet
Custom Implementation of [**Non-deep Networks**](https://arxiv.org/abs/2110.07641) <br>

***arXiv:2110.07641*** <br>
[Ankit Goyal](http://imankgoyal.github.io), [Alexey Bochkovskiy](http://www.alexeyab.com/), [Jia Deng](https://www.cs.princeton.edu/~jiadeng/), [Vladlen Koltun](http://vladlen.info/)<br/>

Official Repository https://github.com/imankgoyal/NonDeepNetworks

<div align="justify">
<i><b>Overview</b>: Depth is the hallmark of DNNs. But more depth means more sequential computation and higher latency. This begs the question -- is it possible to build high-performing ``non-deep" neural networks? We show that it is. We show, for the first time, that a network with a depth of just 12 can achieve top-1 accuracy over 80% on ImageNet, 96% on CIFAR10, and 81% on CIFAR100. We also show that a network with a low-depth (12) backbone can achieve an AP of 48% on MS-COCO.</i>
</div><br>

<img src="img/ParNet.PNG" align="center" width="100%"/>


<div align="justify">
  <i><b>If there are any issues in the code, please feel free to update.</b>
</div><br>
